<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Project 2 - Fun with Filters and Frequencies</title>
    <link rel="stylesheet" href="../../style.css" />
    <script
      src="https://kit.fontawesome.com/bde765b426.js"
      crossorigin="anonymous"
    ></script>
  </head>
  <body>
    <header class="navbar">
      <div class="logo">CS-180</div>
      <button class="toggle-mode">
        <i class="fa-regular fa-sun fa-sm sun-icon"></i>
        <i class="fa-regular fa-moon fa-sm moon-icon"></i>
      </button>
    </header>

    <div class="container">
      <nav class="sidebar">
        <ul>
          <li><a href="#section1">Introduction</a></li>

          <li><a href="#section2">Finite Difference Operator</a></li>
          <li><a href="#section2.1" class="sub">Approach</a></li>
          <li><a href="#section2.2" class="sub">Result</a></li>

          <li><a href="#section3">Derivative of Gaussian (DoG) Filter</a></li>

          <li><a href="#section4">Image "Sharpening"</a></li>
          <li><a href="#section4.1" class="sub">Taj Mahal</a></li>
          <li><a href="#section4.2" class="sub">Camera Obscura</a></li>
          <li><a href="#section4.3" class="sub">Swedish Midsummer ðŸ‡¸ðŸ‡ª</a></li>
          <li><a href="#section4.4" class="sub">Lena</a></li>

          <li><a href="#section5">Hybrid Images</a></li>
          <li><a href="#section5.1" class="sub">Approach</a></li>
          <li><a href="#section5.2" class="sub">Result</a></li>
          <li><a href="#section5.3" class="sub">Steve Jozniak</a></li>
          <li><a href="#section5.4" class="sub">Fourier Analysis</a></li>
          <li><a href="#section5.5" class="sub">Failure</a></li>

          <li><a href="#section6">Gaussian and Laplacian Stacks</a></li>
          <li><a href="#section6.1" class="sub">Approach</a></li>
          <li><a href="#section6.2" class="sub">Result</a></li>
          <li><a href="#section6.3" class="sub">Verification</a></li>

          <li><a href="#section7">Multiresolution Blending</a></li>
          <li><a href="#section7.1" class="sub">Approach</a></li>
          <li><a href="#section7.2" class="sub">Result (Orapple)</a></li>
          <li><a href="#section7.3" class="sub">Irregular mask</a></li>
          <li><a href="#section7.4" class="sub">Horizontal mask</a></li>
        </ul>
      </nav>

      <main class="content">
        <section id="section1">
          <h1>Project 2 - Fun with Filters and Frequencies</h1>
          <p>Filip Malm-BÃ¤gÃ©n</p>
          <hr />
          <img
            id="hero"
            src="../data/basketball_face.png"
            alt="Blended image of a face and a basketball"
          />
          <h2>Introduction</h2>
          <p>
            This project explores using frequencies to process and combine
            images. The project shows the process and result of sharpening
            images by emphasizing high frequencies, extracting edges with finite
            difference kernels, creating hybrid images by blending high and low
            frequencies from different images, and blending images at various
            frequencies using Gaussian and Laplacian stacks.
          </p>
        </section>

        <section id="section2">
          <h2>Finite Difference Operator</h2>
        </section>

        <section id="section2.1">
          <h3>Approach</h3>
          <p>
            To compute the partial derivatives in the x and y directions of the
            "cameraman" image, I first created finite difference kernels as
            Numpy arrays: <code>D_x = np.array([[1, -1]])</code> and
            <code>y = np.array([[1], [-1]])</code>. Using
            <code>scipy.signal.convolve2d</code> with <code>mode='same'</code>,
            I convolved the image with these kernels to obtain the partial
            derivative images, which represent the changes in pixel intensity in
            the x and y directions, respectively. Thereafter, I computed the
            gradient magnitude image using the formula
            <code
              >np.sqrt(partial_derivative_x ** 2 + partial_derivative_y **
              2)</code
            >, which combines the two partial derivatives to highlight the edge
            strength at each pixel. To create an edge image, I applied a
            threshold to the gradient magnitude image,
            <code>Threshold = 0.2</code>. Selecting the threshold value through
            trial and error to balance noise suppression with the visibility of
            real edges.
          </p>
        </section>

        <section id="section2.2">
          <h3>Result</h3>

          <figure>
            <img
              src="./result/partial_derivatives.png"
              alt="Partial derivatives of the cameraman image"
            />
            <figcaption>Partial derivatives of the cameraman image</figcaption>
          </figure>

          <figure>
            <img
              src="./result/gradient_magnitude.png"
              alt="Gradient magnitude image of the cameraman"
            />
            <figcaption>Gradient magnitude image of the cameraman</figcaption>
          </figure>
        </section>

        <section id="section3">
          <h2>Derivative of Gaussian (DoG) Filter</h2>
          <p>
            First, the cameraman is blurred using a Gaussian filter, made using
            <code>cv2.getGaussianKernel</code> with kernel size
            <code>6</code> and sigma <code>1.0</code>. Afterwards, the image
            gradient magnitude of the blurred image is computed using the same
            method as in the previous section. Finally, the blurry gradient
            magnitude image is thresholded to create an edge image. The
            threshold value was set to <code>0.05</code> in order to correspond
            to the result of the previous binarized cameraman.
          </p>

          <p>
            There is a clear difference in the final result. The most obvious
            one is that the edges are thicker for the binarized edges and
            rounder that previously.
          </p>

          <figure>
            <img
              src="./result/partial_derivatives_of_blurred.png"
              alt="Partial derivatives of the blurred cameraman image"
            />
            <figcaption>
              Partial derivatives of the blurred cameraman image
            </figcaption>
          </figure>

          <figure>
            <img
              src="./result/grad_mag_bin_grad.png"
              alt="Gradient magnitude image of the blurred cameraman"
            />
            <figcaption>
              Gradient magnitude image of the blurred cameraman
            </figcaption>
          </figure>

          <figure>
            <img
              src="./result/comparason_bin.png"
              alt="Comparison of binarized gradient magnitude images"
            />
            <figcaption>
              Comparison of binarized gradient magnitude images
            </figcaption>
          </figure>

          <p>
            The two images are essentially the same. If looking closely, the
            grass and other small details differ, but the overall image is the
            same.
          </p>
        </section>

        <section id="section4">
          <h2>Image "Sharpening"</h2>
          <p>
            To sharpen an image, the image was first convolved with a Gaussian
            filter, to filter out the high frequencies, which resulted in a
            blurry image. The high frequencies were then extracted by
            subtracting the blurry image from the original image. Finally, the
            high frequencies were added back to the original image to create a
            sharpened image, using
            <code>sharpened_img = img + alpha * details</code>, where
            <code>alpha</code> is a constant sharpening factor.
          </p>
        </section>

        <section id="section4.1">
          <h3>Taj Mahal</h3>
          <p>
            The Taj Mahal image was sharpened using a sharpening factor of
            <code>0.75</code>. As seen, the sharpened image has more defined
            edges and details compared to the original image.
          </p>

          <figure>
            <img
              src="./result/sharpened_taj.png"
              alt="Sharpened image of the Taj Mahal"
            />
            <figcaption>Sharpened image of the Taj Mahal</figcaption>
          </figure>
        </section>

        <section id="section4.2">
          <h3>Camera Obscura</h3>
          <p>
            A couple of weeks ago, I had the opportunity to visit the "Camera
            Obscura & Holograph Gallery" in San Francisco. It was very
            interesing to see how the camera obscura works and how it can be
            used to create images. Unfortunately, the resulting image lacked
            sharpness... Luckily, I now know an algorithm to sharpen images! I
            used <code>alpha = 6.0</code> and the resulting image is sharper
            than ever. The edges around the horizon and windows are much more
            defined, but the image is also noisier.
          </p>

          <figure>
            <img
              src="./result/camera_obscura.png"
              alt="Sharpened image of the Camera Obscura"
            />
            <figcaption>Sharpened image of the Camera Obscura</figcaption>
          </figure>
        </section>

        <section id="section4.3">
          <h3>Swedish Midsummer ðŸ‡¸ðŸ‡ª</h3>
          <p>
            In Sweden, Midsummer is the biggest holiday of the year. We eat a
            lot of pickled herring and strawberries, and we celebrate all day
            and all night, usually up until the sun rises again. I captured this
            image of my friends dancing at midnight, but due to the lack of
            light, the image turned out blurry... But by sharpening the image
            with <code>alpha = 2.0</code>, the image is now a bit clearer. The
            image lacks information to begin with (I can't enhance something
            which does not exist), but the edges are more defined than before.
          </p>

          <figure>
            <img
              src="./result/midsummer.png"
              alt="Sharpened image of Swedish Midsummer celebration"
            />
            <figcaption>
              Sharpened image of Swedish Midsummer celebration
            </figcaption>
          </figure>
        </section>

        <section id="section4.4">
          <h3>Lena</h3>
          <p>
            Finally, the Lena image was first blurred using a Gaussian filter
            with <code>kernel = 15</code> and <code>sigma = 2.0</code>.
            Thereafter, I sharpened the image using <code>alpha = 4.0</code>,
            with the ambition to make the sharpened image look like the original
            image. The sharpened image looks somewhat similar to the original
            image. As a measure of similarity, I computed the mean squared error
            (MSE) and the Structural Similarity Index (SSIM) between the
            original and sharpened images. MSE measures the average squared
            difference between the two images. A lower MSE indicates a closer
            match between the images. SSIM is a perceptual metric that measures
            the similarity between two images. A higher SSIM indicates a closer
            match between the images and 0 indicates no similarity. The MSE and
            SSIM values were <code>19 766</code> and <code>0.0046</code>,
            respectively. The values indicates that the sharpened image is not
            similar to the original image at all. This might happen because
            sharpening can create new edges and artifacts, making the pixel
            values differ a lot. As a result, the metrics show low similarity,
            even if the image looks somewhat similar to you.
          </p>

          <figure>
            <img src="./result/lena.png" alt="Sharpened image of Lena" />
            <figcaption>Sharpened image of Lena</figcaption>
          </figure>
        </section>

        <section id="section5">
          <h2>Hybrid Images</h2>
        </section>

        <section id="section5.1">
          <h3>Approach</h3>
          <p>
            Two images are taken as input: <code>im1</code> and
            <code>im2</code>. A Gaussian blur is applied to
            <code>im2</code> using <code>sigma2</code> to produce the
            low-frequency image <code>low_frequencies</code>. For the higher
            frequencies, a Gaussian blur is applied to <code>im1</code> using
            <code>sigma1</code> to produce <code>blurred_im1</code>, and the
            high frequencies are extracted as
            <code>high_frequencies = im1 - blurred_im1</code>. Finally,
            <code>low_frequencies</code> and <code>high_frequencies</code> are
            added together pixel-wise to produce the hybrid image.
          </p>

          <p>
            The high pass filter has a sigma of <code>3</code> and the low pass
            filter has a sigma of <code>10</code>.
          </p>
        </section>

        <section id="section5.2">
          <h3>Result</h3>
          <p></p>

          <div style="display: flex; justify-content: space-around">
            <figure style="width: 260px">
              <img src="../data/DerekPicture.jpg" alt="Image of Derek" />
              <figcaption>Image of Derek</figcaption>
            </figure>

            <figure style="width: 500px">
              <img src="../data/nutmeg.jpg" alt="Image of Nutmeg" />
              <figcaption>Image of Nutmeg</figcaption>
            </figure>

            <figure style="width: 300px">
              <img
                src="./result/derek_nutmeg.png"
                alt="Hybrid image of Derek and Nutmeg"
              />
              <figcaption>Hybrid image of Derek and Nutmeg</figcaption>
            </figure>
          </div>

          <p>
            I also experimented with removing the color to see if the effect
            would be the better or worse. On this image with these specific
            values, it seems like the effect is better when the images are in
            black and white. The colors of Derek was too strong in comparason to
            the high frequencies of Nutmeg. The hybrid image in black and white
            is more balanced.
          </p>

          <figure>
            <img
              src="./result/derek_nutmeg_bw.png"
              alt="Hybrid image of Derek and Nutmeg in black and white"
            />
            <figcaption>
              Hybrid image of Derek and Nutmeg in black and white
            </figcaption>
          </figure>
        </section>

        <section id="section5.3">
          <h3>Steve Jozniak</h3>
          <p>
            I also tried to create a hybrid image of Steve Jobs and Steve Jobs.
            The high pass filter has a sigma of <code>1</code> and the low pass
            filter has a sigma of <code>5</code>.
          </p>
          <figure>
            <img
              src="./result/steve_jozniac.png"
              alt="Image of Steve Jozniak"
            />
            <figcaption>Image of Steve Jozniak</figcaption>
          </figure>
        </section>

        <section id="section5.4">
          <h3>Fourier Analysis</h3>
          <p>
            For the Steve Jozniak image, I performed a Fourier transforms to the
            original input images, the filtered images, and the final hybrid
            image. This gave the following results.
          </p>

          <figure>
            <img
              src="./result/fourier_analysis.png"
              alt="Fourier analysis of Steve Jozniak hybrid image"
            />
            <figcaption>
              Fourier analysis of Steve Jozniak hybrid image
            </figcaption>
          </figure>
        </section>

        <section id="section5.5">
          <h3>Failure</h3>
          <p>
            I tried to create this hybrid image where it says "Hello" when the
            reader is close to the image and "Adios" when the reader is far
            away. I experimented some with the sigma values but it did not look
            correct. I belive it can be due to the thin lines. A more bold font
            migh have been better.
          </p>

          <div style="display: flex; justify-content: space-around">
            <figure style="width: 33%">
              <img src="../data/hello.png" alt="Image with the text 'Hello'" />
              <figcaption>Image with the text 'Hello'</figcaption>
            </figure>

            <figure style="width: 33%">
              <img src="../data/adios.png" alt="Image with the text 'Adios'" />
              <figcaption>Image with the text 'Adios'</figcaption>
            </figure>

            <figure style="width: 33%">
              <img src="./result/hybrid_fail.png" alt="Failed hybrid image" />
              <figcaption>Failed hybrid image</figcaption>
            </figure>
          </div>
        </section>

        <section id="section6">
          <h2>Gaussian and Laplacian Stacks</h2>
        </section>

        <section id="section6.1">
          <h3>Approach</h3>
          <p>
            Gaussian stacks are generated for each channel of the color images
            by progressively applying Gaussian blur without downsampling,
            ensuring that the image size remains constant across all levels. The
            Laplacian stack is then computed by subtracting consecutive levels
            of the Gaussian stack, capturing the details lost between the
            blurred versions at each level, while the final level is the most
            blurred image from the Gaussian stack. This process is performed
            separately for each channel of the RGB image, and the stacks are
            combined to produce multi-level representations that preserve both
            fine and coarse image details.
          </p>
        </section>

        <section id="section6.2">
          <h3>Result</h3>
          <figure>
            <img
              src="./result/l_stack_apple.png"
              alt="Laplacian stack of an apple image"
            />
            <figcaption>
              Laplacian stack of an apple image using
              <code>levels = 5</code> and <code>sigma = 2</code>
            </figcaption>
          </figure>

          <figure>
            <img
              src="./result/l_stack_orange.png"
              alt="Laplacian stack of an orange image"
            />
            <figcaption>
              Laplacian stack of an orange image using
              <code>levels = 5</code> and <code>sigma = 2</code>
            </figcaption>
          </figure>
        </section>

        <section id="section6.3">
          <h3>Verification</h3>
          <p>
            To verify that the algorithm works, the image is reconstructed from
            the Laplacian stack by progressively adding each level, starting
            from the most blurred image at the bottom of the stack and moving
            upwards. The reconstruction process ensures that lost details at
            each level are reintroduced, with pixel values being clipped to stay
            within the valid range.
          </p>

          <div style="display: flex; justify-content: space-around">
            <figure>
              <img
                src="./result/apple_reconstruct.png"
                alt="Reconstructed image of an apple"
              />
              <figcaption>Reconstructed image of an apple</figcaption>
            </figure>

            <figure>
              <img
                src="./result/orange_reconstruct.png"
                alt="Reconstructed image of an orange"
              />
              <figcaption>Reconstructed image of an orange</figcaption>
            </figure>
          </div>
        </section>

        <section id="section7">
          <h2>Multiresolution Blending</h2>
        </section>

        <section id="section7.1">
          <h3>Approach</h3>
          <p>
            Two images are seamlessly blended using Laplacian stacks and a mask.
            A <code>gaussian_stack</code> is first generated for both images and
            the mask to capture details at different levels. The
            <code>laplacian_stack</code> is then computed by subtracting
            adjacent Gaussian levels, highlighting high-frequency details. The
            mask, transitioning from 1 to 0, is blurred to create a smooth
            blend. The stacks are combined using
            <code>blend_laplacian_stacks</code>, and the final image is
            reconstructed with <code>reconstruct_image</code>, progressively
            adding the levels back.
          </p>
        </section>

        <section id="section7.2">
          <h3>Result (Orapple)</h3>
          <p>
            <code>levels = 4</code>, <code>sigma = 1.0</code> and
            <code>sigma_mask = 64</code>
          </p>
          <div style="text-align: center">
            <figure>
              <img
                src="./result/blended.png"
                alt="Blended image of an apple and an orange"
              />
              <figcaption>Blended image of an apple and an orange</figcaption>
            </figure>
          </div>
        </section>

        <section id="section7.3">
          <h3>Irregular mask</h3>
          <p>
            Here i tried to place my face on a basketball. I chose a basketball
            because I though a basketball had the most similar color to my
            skintone. <code>levels = 8</code>, <code>sigma = 20</code> and
            <code>sigma_code = 8</code>.
          </p>

          <div style="display: flex; justify-content: space-around">
            <figure style="width: 400px">
              <img src="../data/filip.jpg" alt="Image of me" />
              <figcaption>Image of me</figcaption>
            </figure>

            <figure style="width: 400px">
              <img src="../data/mask.png" alt="Mask used for blending" />
              <figcaption>Mask used for blending</figcaption>
            </figure>

            <figure>
              <img
                src="./result/basket_blended.png"
                alt="Blended image of my face and a basketball"
              />
              <figcaption>Blended image of my face and a basketball</figcaption>
            </figure>
          </div>
        </section>

        <section id="section7.4">
          <h3>Horizontal mask</h3>
          <p>
            For this image, I blended a cactus and an ice cream using a
            horizontal mask.
            <code>levels = 6</code>, <code>sigma = 0.4</code> and
            <code>sigma_mask = 2</code>
          </p>

          <div style="text-align: center">
            <figure>
              <img
                src="./result/cactus_ice_cream.png"
                alt="Blended image of a cactus and an ice cream"
              />
              <figcaption>
                Blended image of a cactus and an ice cream
              </figcaption>
            </figure>
          </div>
        </section>

        <p>
          <i>This webpage design was partly made using generative AI models.</i>
        </p>
      </main>
    </div>

    <script>
      const toggleButton = document.querySelector('.toggle-mode');
      const body = document.body;

      // Function to set the appropriate icon based on the mode
      function updateIcons() {
        if (body.classList.contains('dark-mode')) {
          document.querySelector('.sun-icon').style.display = 'none';
          document.querySelector('.moon-icon').style.display = 'inline-block';
        } else {
          document.querySelector('.sun-icon').style.display = 'inline-block';
          document.querySelector('.moon-icon').style.display = 'none';
        }
      }

      // Check if dark mode is enabled in localStorage when page loads
      if (localStorage.getItem('dark-mode') === 'enabled') {
        body.classList.add('dark-mode');
        updateIcons();
      } else {
        updateIcons(); // Ensure the icons are set correctly for light mode
      }

      // Toggle between light and dark mode on button click
      toggleButton.addEventListener('click', () => {
        body.classList.toggle('dark-mode');

        // Save the dark mode state to localStorage
        if (body.classList.contains('dark-mode')) {
          localStorage.setItem('dark-mode', 'enabled');
        } else {
          localStorage.removeItem('dark-mode');
        }

        // Update the icons when the mode is toggled
        updateIcons();
      });
    </script>
  </body>
</html>
