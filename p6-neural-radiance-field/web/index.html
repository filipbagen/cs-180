<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Project 5 - Diffusion Models</title>
    <link rel="stylesheet" href="../../style.css" />
    <script
      src="https://kit.fontawesome.com/bde765b426.js"
      crossorigin="anonymous"
    ></script>
  </head>
  <body>
    <header class="navbar">
      <div class="logo">CS-180</div>
      <button class="toggle-mode">
        <i class="fa-regular fa-sun fa-sm sun-icon"></i>
        <i class="fa-regular fa-moon fa-sm moon-icon"></i>
      </button>
    </header>

    <div class="container">
      <nav class="sidebar">
        <ul>
          <li><a href="#section0">Introduction</a></li>
          <li>
            <a href="#section1">Part 1: Fit a Neural Field to a 2D Image</a>
          </li>
          <li>
            <a href="#section2"
              >Part 2: Fit a Neural Radiance Field from Multi-view Images</a
            >
          </li>
          <li><a href="#section21">Part 2.1: Create Rays from Cameras</a></li>
          <li><a href="#section22">Part 2.2: Sampling</a></li>
          <li>
            <a href="#section23"
              >Part 2.3: Putting the dataloader all together</a
            >
          </li>
          <li><a href="#section24">Part 2.4: Neural Radiance Field</a></li>
          <li><a href="#section25">Part 2.5: Volume Rendering</a></li>
        </ul>
      </nav>

      <main class="content">
        <section id="section0">
          <h1>Final Project - Neural Radiance Field</h1>
          <p>Filip Malm-Bägén</p>
          <hr />
          <img
            id="hero"
            src="../code/results/nerf.gif"
            alt="Hero"
            loop="infinite"
          />
          <h2>Introduction</h2>
          <p>
            This project is about implementing neural radiance fields (NeRFs) to
            reconstruct 3D scenes from 2D images by sampling and querying rays
            to integrate color information. The goal is to build efficient and
            robust methods for ray generation, sampling, and rendering to ensure
            high-quality results while minimizing overfitting.
          </p>
        </section>

        <!-- Part 1 -->
        <section id="section1">
          <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
          <p>
            The main task was to fit a neural field <code>F</code> that maps 2D
            pixel coordinates <code>{u, v}</code> to RGB color values
            <code>{r, g, b}</code>. To achieve this, I implemented a
            <code>Multi_Layer_Perceptron</code> class with the following
            architecture:
          </p>

          <figure style="text-align: center">
            <img
              src="../data/result/2dmlp.png"
              alt="MLP Architecture"
              style="height: 200px"
            />
            <figcaption>2D MLP Architecture</figcaption>
          </figure>

          <p>
            The inputs to the network were augmented using Sinusoidal Positional
            Encoding (PE) using <code>get_PE</code>, which expands the input
            dimensionality from 2 to <code>4 * L + 2</code> based on the
            frequency level <code>L</code>. To train on large images, I had to
            implement a dataloader class, <code>RandomPixelSampler</code>, which
            samples <code>N</code> random pixels per iteration and returns both
            their normalized coordinates and RGB values.
          </p>
          <p>
            During the last step, I experimented with different hyperparameters
            to see how the results was effected. As seen in the images, the base
            configuration performed the best. The reduced frequency made the fox
            look more smooth and cartoonish. This is no surprise since the
            reduced frequency makes the network less sensitive to high-frequency
            details. The wider configuration and higher learning rate did
            perform the worst, as they resulted in black images. This result is
            also confirmed by the PSNR graph. However, this was not the case for
            the palace image, where the wider configuration performed the best.
            This is likely due to the increased complexity of the image.
          </p>

          <!-- Table -->
          <table>
            <caption>
              Neural Field Network Configurations and Hyperparameters
            </caption>

            <tr>
              <th>Configuration</th>
              <th>Channel Size</th>
              <th>Learning Rate</th>
              <th>L</th>
            </tr>
            <tr>
              <td>Base</td>
              <td>256</td>
              <td>1e-2</td>
              <td>10</td>
            </tr>
            <tr>
              <td>Reduced Freq</td>
              <td>256</td>
              <td>1e-2</td>
              <td>3 (Reduced frequency)</td>
            </tr>
            <tr>
              <td>Wider</td>
              <td>512 (Double channel size)</td>
              <td>1e-2</td>
              <td>10</td>
            </tr>
            <tr>
              <td>Higher LR</td>
              <td>256</td>
              <td>1e-1 (Higher learning rate)</td>
              <td>10</td>
            </tr>
          </table>

          <figure style="text-align: center">
            <img
              src="../data/fox.jpg"
              alt="Original fox image"
              style="height: 200px"
            />
            <figcaption>Original fox image</figcaption>
          </figure>

          <figure>
            <img
              src="../data/result/base_fox.png"
              alt="Base configuration on fox"
            />
            <figcaption>Base configuration on fox</figcaption>

            <img
              src="../data/result/reduced_freq_fox.png"
              alt="Reduced frequency on fox"
            />
            <figcaption>Reduced frequency on fox</figcaption>

            <img src="../data/result/wider_fox.png" alt="Wider on fox" />
            <figcaption>Wider on fox</figcaption>

            <img
              src="../data/result/higher_lr_fox.png"
              alt="Higher learning rate on fox"
            />
            <figcaption>Higher learning rate on fox</figcaption>
          </figure>
          x

          <p>The images represent iteration [1, 20, 100, 500, 1000, 2000]</p>

          <figure style="text-align: center">
            <img src="../data/result/PSNR_vs_iter_fox.png" alt="PSNR fox" />
            <figcaption>PSNR for different configurations for fox</figcaption>
          </figure>

          <figure style="text-align: center">
            <img
              src="../data/palace_of_fine_arts.jpeg"
              alt="Original Palace of Fine Arts image"
              style="height: 200px"
            />
            <figcaption>Original Palace of Fine Arts image</figcaption>
          </figure>

          <figure>
            <img
              src="../data/result/base_palace.png"
              alt="Base configuration on palace"
            />
            <figcaption>Base configuration on palace</figcaption>

            <img
              src="../data/result/reduced_freq_palace.png"
              alt="Reduced frequency on palace"
            />
            <figcaption>Reduced frequency on palace</figcaption>

            <img src="../data/result/wider_palace.png" alt="Wider on palace" />
            <figcaption>Wider on palace</figcaption>

            <img
              src="../data/result/higher_lr_palace.png"
              alt="Higher learning rate on palace"
            />
            <figcaption>Higher learning rate on palace</figcaption>
          </figure>

          <figure>
            <img
              src="../data/result/PSNR_vs_iter_palace.png"
              alt="PSNR palace"
            />
            <figcaption>
              PSNR for different configurations for palace
            </figcaption>
          </figure>

          <p>
            The reconstruction quality was evaluated using the Peak
            Signal-to-Noise Ratio (PSNR), calculated as
            <code>PSNR = 10 * log10(1 / MSE)</code>
          </p>
        </section>

        <!-- Part 2 -->
        <section id="section2">
          <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
          <p>
            This part involves using a Neural Radiance Field (NeRF) to represent
            a 3D scene by learning a mapping from position and view direction to
            color and density:
            <code>F: {x, y, z, d} → {r, g, b, σ}</code>. Using multi-view
            calibrated images of a Lego scene (200x200 resolution) and their
            corresponding camera poses, the task aims to perform inverse
            rendering. The provided data includes camera-to-world matrices for
            training, validation, and test cameras.
          </p>
        </section>

        <section id="section21">
          <h3>Part 2.1: Create Rays from Cameras</h3>
          <p>
            To render the 3D scene, I implemented functions to convert pixel
            coordinates into rays, defined by their origin (<code
              >r<sub>o</sub></code
            >) and normalized direction (<code>r<sub>d</sub></code
            >). The coordinate transformations are performed and a function
            transforms camera coordinates to world coordinates using the
            camera-to-world matrix. Another function transforms pixel
            coordinates to camera coordinates using the intrinsic matrix and
            pixel depth. For ray generation, the ray origin is the camera's
            translation vector, and the direction is computed by normalizing the
            difference between the world coordinate of a depth-1 point and the
            origin. These transformations are implemented using batched matrix
            multiplications for efficiency.
          </p>
        </section>

        <section id="section22">
          <h3>Part 2.2: Sampling</h3>
          <p>
            Next, I developed ray sampling methods. I trained the model using a
            batch size of 10k rays. These rays were generated by randomly
            sampling 10k pixels globally across the training set of 100 images.
            To accelerate the training process, all rays and pixel coordinates
            were precomputed at the start. To render the 3D scene, each ray was
            discretized into sampled points along its path. This step allows
            querying points to integrate their colors for determining the final
            color rendered at a particular pixel. Using uniform sampling, I
            generated points along each ray as:
            <code>t = np.linspace(near, far, n_samples)</code>, where
            <code>near=2.0</code>, <code>far=6.0</code>, and
            <code>n_samples=64</code>. The 3D coordinates for these points were
            calculated as: <code>x = r_o + r_d * t</code>, where
            <code>r_o</code> represents the ray origin, and <code>r_d</code> the
            ray direction. I added perturbations during training,
            <code>t = t + np.random.rand(t.shape) * t_width</code>. This ensures
            that all locations along the ray are touched.
          </p>
        </section>

        <section id="section23">
          <h3>Part 2.3: Putting the dataloader all together</h3>
          <p>
            I ran the code to verify that I had implement everything correctly.
          </p>

          <figure>
            <img
              src="../code/results/viser.png"
              alt="100 Randomly Sampled Rays"
            />
            <figcaption>100 Randomly Sampled Rays</figcaption>
          </figure>
        </section>

        <section id="section24">
          <h3>Part 2.4: Neural Radiance Field</h3>
          <p>
            The Neural Radiance Field was implemented as a deep neural network
            that maps spatial coordinates and viewing directions to color and
            density values. This network was enhanced to handle
            higher-dimensional inputs (3D position and view direction vectors)
            and outputs (RGB colors plus density), compared to the MLP from part
            1. The complete network architecture is illustrated below:
          </p>

          <figure>
            <img src="../data/result/mlp_nerf.png" alt="3D MLP Architecture" />
            <figcaption>3D MLP Architecture</figcaption>
          </figure>
        </section>

        <section id="section25">
          <h3>Part 2.5: Volume Rendering</h3>
          <p>
            Volume rendering integrates color values along each ray to produce
            the final pixel color. At each sampled point along a ray, the
            network predicts both color and density values. These values are
            then combined using a numerical approximation of the volume
            rendering equation. The rendering process works by accumulating
            colors from back to front along each ray, using density values (σ)
            to determine opacity at each point, weighting colors based on
            transmittance (how much light passes through), and computing
            distance intervals (δᵢ) between sampled points.
          </p>

          <p>
            The implementation uses PyTorch's <code>torch.cumprod</code> for
            efficient calculation of transmittance values. The distance
            intervals δᵢ are derived from the sampling points generated earlier
            in the pipeline. This numerical approximation enables efficient
            parallel computation across all rays in a batch.
          </p>

          <div class="equation">
            \[\begin{align} \hat{C}(\mathbf{r})=\sum_{i=1}^N T_i\left(1-\exp
            \left(-\sigma_i \delta_i\right)\right) \mathbf{c}_i, \text{ where }
            T_i=\exp \left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)
            \end{align}\]
          </div>

          <p>
            I trained using Adam optimizer with
            <code>learning_rate = 1e-3</code>,
            <code>batch_size = 10_000</code> and <code>iterations = 2500</code>.
            The images below are visualizations of the training process.
          </p>

          <figure>
            <div
              style="
                display: grid;
                grid-template-columns: repeat(4, 1fr);
                gap: 10px;
              "
            >
              <div style="text-align: center">
                <img
                  src="../code/results/depth/render_0.jpg"
                  alt="Depth Render 0"
                />
                <figcaption>Depth Render 0</figcaption>
              </div>
              <div style="text-align: center">
                <img
                  src="../code/results/depth/render_10.jpg"
                  alt="Depth Render 10"
                />
                <figcaption>Depth Render 10</figcaption>
              </div>
              <div style="text-align: center">
                <img
                  src="../code/results/depth/render_20.jpg"
                  alt="Depth Render 20"
                />
                <figcaption>Depth Render 20</figcaption>
              </div>
              <div style="text-align: center">
                <img
                  src="../code/results/depth/render_40.jpg"
                  alt="Depth Render 40"
                />
                <figcaption>Depth Render 40</figcaption>
              </div>
              <div style="text-align: center">
                <img
                  src="../code/results/final_render/render_0.jpg"
                  alt="Final Render 0"
                />
                <figcaption>Final Render 0</figcaption>
              </div>
              <div style="text-align: center">
                <img
                  src="../code/results/final_render/render_10.jpg"
                  alt="Final Render 10"
                />
                <figcaption>Final Render 10</figcaption>
              </div>
              <div style="text-align: center">
                <img
                  src="../code/results/final_render/render_20.jpg"
                  alt="Final Render 20"
                />
                <figcaption>Final Render 20</figcaption>
              </div>
              <div style="text-align: center">
                <img
                  src="../code/results/final_render/render_40.jpg"
                  alt="Final Render 40"
                />
                <figcaption>Final Render 40</figcaption>
              </div>
            </div>
          </figure>

          <p>
            The following image is the PSNR curve over the iterations. The PSNR
            is steadily increasing, and I believe that the model could acieve a
            PSNR greater than 30 if trained for more iterations.
          </p>

          <figure style="text-align: center">
            <img src="../code/results/plot.png" alt="PSNR NeRF" />
            <figcaption>PSNR for NeRF</figcaption>
          </figure>

          <p>
            The final image is the rendered image of the Lego scene. The
            rendering quality is quite good, and the model has learned to
            represent the scene well. Next to it is the depth image, which
            represents the distance from the camera to the object. The darker
            the pixel, the closer it is to the camera. The difference is that
            the depth rendering only uses the density value, while the color
            rendering uses both the color and density values.
          </p>

          <figure style="display: flex; justify-content: center; gap: 20px">
            <div>
              <img src="../code/results/nerf.gif" alt="NeRF Render" />
              <figcaption>NeRF Render</figcaption>
            </div>
            <div>
              <img src="../code/results/depth.gif" alt="NeRF Depth" />
              <figcaption>NeRF Depth</figcaption>
            </div>
          </figure>
        </section>

        <p>
          <i>This webpage design was partly made using generative AI models.</i>
        </p>
      </main>
    </div>

    <script>
      const toggleButton = document.querySelector('.toggle-mode');
      const body = document.body;

      // Function to set the appropriate icon based on the mode
      function updateIcons() {
        if (body.classList.contains('dark-mode')) {
          document.querySelector('.sun-icon').style.display = 'none';
          document.querySelector('.moon-icon').style.display = 'inline-block';
        } else {
          document.querySelector('.sun-icon').style.display = 'inline-block';
          document.querySelector('.moon-icon').style.display = 'none';
        }
      }

      // Check if dark mode is enabled in localStorage when page loads
      if (localStorage.getItem('dark-mode') === 'enabled') {
        body.classList.add('dark-mode');
        updateIcons();
      } else {
        updateIcons(); // Ensure the icons are set correctly for light mode
      }

      // Toggle between light and dark mode on button click
      toggleButton.addEventListener('click', () => {
        body.classList.toggle('dark-mode');

        // Save the dark mode state to localStorage
        if (body.classList.contains('dark-mode')) {
          localStorage.setItem('dark-mode', 'enabled');
        } else {
          localStorage.removeItem('dark-mode');
        }

        // Update the icons when the mode is toggled
        updateIcons();
      });
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </body>
</html>
